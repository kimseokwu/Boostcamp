{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망을 수식으로 분해해보자\n",
    "\n",
    "- 비선형모델 신경망(neural network)\n",
    "- $\\vec{O} = \\vec{X}\\vec{W}+\\vec{b}$\n",
    "- $d$개의 잠재 변수로 $p$개의 선형모델을 만들어서 $p$개의 잠재변수를 설명하는 모델을 상상해볼 수 있습니다.\n",
    "- 출력 벡터 $O$에 softmax 함수를 합성하면 확률벡터가 되므로 특정 클래스 $k$에 속할 확률을 계산할 수 있다.\n",
    "\n",
    "__softmax 연산__\n",
    "\n",
    "- 소프트맥스 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산입니다.\n",
    "- 분류 문제를 풀때 선형모델과 소프트맥스 함수를 결합하여 예측합니다.\n",
    "\n",
    "$$\n",
    "softmax(O) = (\\frac{exp(o_1)}{\\sum^{p}_{k=1}exp(o_k)} ,\\dots, \\frac{exp(o_p)}{\\sum^{p}_{k=1}exp(o_k)})\n",
    "$$\n",
    "\n",
    "- 그러나 학습이 아니라 추론을 할 때는 원핫벡터로 최대값을 가진 주소만 1로 출력하는 연산을 사용해서 softmax를 사용하진 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],\n",
       "       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],\n",
       "       [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(vec):\n",
    "    #np.max 빼는 이유: 너무 큰 vector가 들어올 때 오버플로우 현상 방지\n",
    "    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))\n",
    "    numerator = np.sum(denumerator, axis=-1, keepdims=True)\n",
    "    val = denumerator / numerator\n",
    "    return val\n",
    "\n",
    "vec = np.array([[1, 2, 0], [-1, 0, 1], [-10, 0, 10]])\n",
    "softmax(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망을 수식으로 분해해보자\n",
    "\n",
    "- 신경망은 선형모델과 활성함수(activation function)를 합성한 함수입니다.\n",
    "\n",
    "- 활성함수는 비선형 함수로써 선형모델로 나오는 출력물에 적용한다.\n",
    "\n",
    "- 활성함수는 vector를 인풋으로 받지 않고 하나의 실수값만 입력으로 받는다.\n",
    "\n",
    "- 활성함수 $\\sigma$는 비선형함수로 잠재벡터 $Z = (z_1,\\dots,z_q)$의 각 노드에 개별적으로 적용하여 새로운 잠재벡터 $H = (\\sigma(z_1), \\dots, \\sigma(z_n))$를 만든다.\n",
    "- 다층(multi-layer) 퍼셉트론(MLP)은 신경망이 여러층 합성된 함수입니다.\n",
    "- 레이어가 순차적인 신경망 계산을 순전파라고 부른다.\n",
    "\n",
    "# 활성함수가 뭐예요?\n",
    "\n",
    "- 활성함수는 $R$ 위에 정의된 비선형 함수로서 딥러닝에서 매우 중요한 개념입니다.\n",
    "- 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없습니다.\n",
    "- 시그모이드 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에선 ReLU 함수를 많이 쓰고 있다.\n",
    "\n",
    "# 왜 층을 여러개를 쌓나요?\n",
    "\n",
    "- 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있습니다.\n",
    "\n",
    "- 그러나 층이 깊을 수록 목적함수를 근사하는 데 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능합니다.\n",
    "\n",
    "- 하지만 층이 깊을수록 최적화는 더 많은 노력을 기울여야한다. 학습이 복잡해진다.\n",
    "\n",
    "# 딥러닝 학습원리: 역전파 알고리즘\n",
    "\n",
    "- 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 패러미터를 학습합니다.\n",
    "\n",
    "- 손실함수를 $L$ 이라 했을 때 역전파는 $\\frac{\\partial L}{\\partial W^{(l)}}$ 정보를 계산할 때 사용된다.\n",
    "\n",
    "- 밑층에 있는 gradient vector를 계산하기 위해서 윗층에 있는 gradient vector가 필요하다.\n",
    "\n",
    "- 각 층 패러미터의 그래디언트 벡터를 계산한 후에 윗층부터 역순으로 연쇄법칙을 이용해 그래디언트 벡터를 전달한다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial O} * \\dots * \\frac{\\partial Z^{(l)}}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "#  역전파 알고리즘 원리 이해하기\n",
    "\n",
    "- 역전파 알고리즘은 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)을 사용합니다.\n",
    "\n",
    "$$\n",
    "z = (x+y)^2 \\\\\n",
    "w = x + y, \\quad\n",
    "z = w^2 \\\\\n",
    "\\frac{\\partial w}{\\partial y} = 1, \\quad\n",
    "\\frac{\\partial z}{\\partial w} = 2w \\\\\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial w}\\frac{\\partial w}{\\partial x} = 2w * 1 = 2(x+y)\n",
    "$$\n",
    "\n",
    "- 각 노드의 텐서 값을 컴퓨터가 기억해야 미분 계산이 가능하다. 따라서 메모리를 더 많이 사용하게 된다.\n",
    "\n",
    "\n",
    "# 예제: 2층 신경망\n",
    "\n",
    "- 2층 신경망의 역전파 알고리즘\n",
    "$\\frac{\\partial L}{\\partial w^{(1)}} = ?$\n",
    "\n",
    "$$\n",
    "O = W^{(2)}h + b^{(2)}\\\\\n",
    "h = \\sigma(z)\\\\\n",
    "z = W^{(1)}x + b^{(1)}\\\\\n",
    "\\frac{\\partial L}{\\partial w^{(1)}} = \\sum \\frac{\\partial L}{\\partial o_l}\\frac{\\partial o_1}{\\partial h_r}\\frac{\\partial h_r}{\\partial z_k}\\frac{\\partial z_k}{\\partial w^{(1)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
